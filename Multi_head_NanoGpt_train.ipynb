{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as f\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "i6kPoU3PNONM"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/combined_harry_potter_books.txt','r',encoding= 'utf-8') as f:\n",
        "  text = f.read()"
      ],
      "metadata": {
        "id": "CncGJoRPPtjE"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text[:500]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "KCcRwYlNPObC",
        "outputId": "8100e8b1-1ce4-44b1-8852-e3dd83fc7cc9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'M r. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people you’d expect to be involved in anything strange or mysterious, because they just didn’t hold with such nonsense.\\n\\nMr. Dursley was the director of a firm called Grunnings, which made drills. He was a big, beefy man with hardly any neck, although he did have a very large mustache. Mrs. Dursley was thin and blonde and had nearly twice the usual amoun'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "id": "_fIKpOfjQM5-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17a3a02b-9855-491a-c207-a77df55a1e0a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.6.15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "tokenizer = tiktoken.get_encoding('gpt2')\n",
        "encoding = tokenizer.encode(text)"
      ],
      "metadata": {
        "id": "ac-1IDSqysIn"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "data = torch.tensor(encoding,dtype = torch.long)"
      ],
      "metadata": {
        "id": "GaS-RrW4zWs7"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[:50]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrTnvJpYQBon",
        "outputId": "3edf7a58-1afc-48f4-a6c0-97da7087dba1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([   44,   374,    13,   290,  9074,    13,   360,  1834,  1636,    11,\n",
              "          286,  1271,  1440,    11,  4389, 16809,  9974,    11,   547,  6613,\n",
              "          284,   910,   326,   484,   547,  7138,  3487,    11,  5875,   345,\n",
              "          845,   881,    13,  1119,   547,   262,   938,   661,   345,   447,\n",
              "          247,    67,  1607,   284,   307,  2950,   287,  1997,  6283,   393])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n  = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "b75iEVyE0BtZ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T0TA50njPppQ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#again bringing the train_data and val_data\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device\n",
        "# so basically apart from increasing the size of the ket, query and values matrix (multi head) we will also add some optimization techniques such as layer norm, dropout neurons stuff like residual connections\n",
        "#increase the no. of transformer blocks to 6 as defined by the n_layer, adding feed forward neural netowrks\n",
        "#so, you need to keep in mind the architecture of transformer while going thru this"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUfYeTVifHW-",
        "outputId": "3270213f-9b83-4056-8836-afd6e52b1158"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_wsd9EObpjni"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# defining the hyperparameters again\n",
        "batch_size = 32 # how many independent sequences will we process in parallel?\n",
        "block_size = 256 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2"
      ],
      "metadata": {
        "id": "xWwDU3R_5u5T"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data loading\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "3gJfFS387D53"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xb,yb = get_batch('train')\n",
        "print('inputs',xb)\n",
        "print('targets',yb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XgubKYM2plLQ",
        "outputId": "1fe2deab-702f-4c6c-b2b4-7ed19bd97c98"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs tensor([[ 3353,   262, 26045,  ...,  2637,   198,   198],\n",
            "        [  257, 17707,  9158,  ..., 14802,  1986,   284],\n",
            "        [ 6504,   276,  1752,  ...,  4656,   717,    13],\n",
            "        ...,\n",
            "        [  251,   198,   198,  ...,   561,   307,   286],\n",
            "        [ 9190,   597,   640,  ..., 39157,    11, 32379],\n",
            "        [45648,   326,   345,  ...,  2368,   614,    11]], device='cuda:0')\n",
            "targets tensor([[  262, 26045,   286,  ...,   198,   198, 18308],\n",
            "        [17707,  9158,  4272,  ...,  1986,   284,   262],\n",
            "        [  276,  1752,   517,  ...,   717,    13,   770],\n",
            "        ...,\n",
            "        [  198,   198,   447,  ...,   307,   286,   502],\n",
            "        [  597,   640,   783,  ...,    11, 32379,  1350],\n",
            "        [  326,   345,   547,  ...,   614,    11,   780]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "@torch.no_grad()\n",
        "#loss evaluation\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "PwHcZKx27LIR"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Head(nn.Module):\n",
        "    #as we have seen it getting implemented in the self attention as well\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # input of size (batch, time-step, channels)\n",
        "        # output of size (batch, time-step, head size)\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,head size)\n",
        "        q = self.query(x) # (B,T,head size)\n",
        "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei =f.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,hs)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
        "        return out"
      ],
      "metadata": {
        "id": "ENQdGAOQ7T_N"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out"
      ],
      "metadata": {
        "id": "-YZJFEb48Fea"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedFoward(nn.Module):\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n"
      ],
      "metadata": {
        "id": "y5RTDyUM8N7k"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "oevBugSK8V3w"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = f.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, loss = self(idx_cond)\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            probs = f.softmax(logits, dim=-1) # (B, C)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n"
      ],
      "metadata": {
        "id": "8Cec5cMK8cCK"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = tokenizer.n_vocab\n",
        "model = GPTModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0i6Rfi5D888v",
        "outputId": "0c52727b-872c-40d4-9cbe-c41f2bbe49cd"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "49.386577 M parameters\n",
            "step 0: train loss 10.8351, val loss 10.8354\n",
            "step 500: train loss 4.0801, val loss 4.4487\n",
            "step 1000: train loss 3.5630, val loss 4.1108\n",
            "step 1500: train loss 3.2359, val loss 4.0209\n",
            "step 2000: train loss 2.9743, val loss 3.9846\n",
            "step 2500: train loss 2.7403, val loss 4.0191\n",
            "step 3000: train loss 2.5341, val loss 4.0936\n",
            "step 3500: train loss 2.3458, val loss 4.1492\n",
            "step 4000: train loss 2.1774, val loss 4.2727\n",
            "step 4500: train loss 2.0008, val loss 4.3763\n",
            "step 4999: train loss 1.8624, val loss 4.4493\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx = torch.zeros((1,1),dtype = torch.long,device = device)\n",
        "generated_text = tokenizer.decode(model.generate(idx,max_new_tokens = 1000)[0].tolist())\n"
      ],
      "metadata": {
        "id": "vci5hTo8PuIN"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text"
      ],
      "metadata": {
        "id": "YCuJK7BL63_P",
        "outputId": "9a0f1bbf-ae49-45b4-98ec-c4169b28f539",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'! . but what happened?”\\n\\n“I asked whether you know about the Stone or not,” said Riddle. “Here. Riddle’s diary, Harry. And I’ve never found out what of Hogwarts— Wouldn’t I open it for the book. Riddle told the first one found that is safe to cross, and — if —”\\n\\n“And — will you speak to me?” Harry said Riddle.\\n\\n“For the Stone!”\\n\\n“Yes, is this very important, very closely,” said Riddle. “But, yes, nothing, there doesn’t do even if I have a Horcrux already, not better.”\\n\\n“So brilliant, toilet, and your soul could on Halloween. I think I knew that — Dumbledore had first entered the forest Vol—”\\n\\n“Then we ran into your office, you use that hundreds of witches and wizards would have slime came. He-Must-Not-Not-Be-Named, as if ever — Voldemort realized ...”\\n\\nAuntie hunting screams had never been involved in this — but the piece of scrapped fell.\\n\\n“Well, I was convinced you, Professor Sages are you and your dad, that Professor Dumbledore must have heard before you, how you know leaving your name in these days?”\\n\\n“Ha!” said Slughorn gencase. “And what do I almost like toproof.”\\n\\n“Look at her!” said Harry in a dangerous voice. “He’ll have to, that’s all, isn’t it? And you’re here.”\\n\\n“No,” said Harry, but not alone, not only much space in the subject at all.\\n\\n“He’s mad,” said Slughorn at once. He told Ron and Hermione what year he had done to the night that he had sat behind. “They haven’t got two Galleons, locket! And have you seen what they’re so fun. wrestling him to Dumbledore! And Hermione and Ginny got me closely followed by cat learn!”\\n\\nThere seemed strange to be a mark on as a surface, but suddenly Harry would never be able to pretend the idea he was relatively short demonstration before, was in shadowioned as anything at the same time at the back of his unexpectedly.\\n\\nExcept looked just as normal as anyone else assembled; bits of parchment had been knocked out by amongst them, and borne they were awake, that they couldn’t bear her eyes. The Marauder’s Map never smelled at hot.\\n\\n“Harry! You weren’t dreaming about all the noise?” Hermione said to Ron and Hermione, sounding slightly terrified. “Hand in something reflected about the way that her.”\\n\\n“I read it up,” said Harry, explaining, “I thought it could be Snape was all aboard! I I’ve fell an pillow. It was curling into the floor and — Goyle whipped around — tears turned down on — and Filch were chucked away from Filch’s corner, they went through a door. Then he heard Neville, pounding Fang’s hairy face, and Ron was gused.\\n\\n“What – I got home for you on our own cloak — that changed —?”\\n\\n“Snape!” Harry whispered. He was wrestling his to the dungeon.\\n\\n“Exactly,” said Ron, setting his lip and Hermione’s utter astonishment, had dropped Harry and his robes toward Snape and Hermione. “It is the Mirror of Erised I decide like to practise, I’m square.”\\n\\nHarry did not think about what he was taking matters brick color.\\n\\n“I’m brilliant,” he said in a hoarse voice, taking a deep breaths. “I can ask you to know, Harry. Can’t you see what I’re thinking of myself without your father, I should have to make your head project to see it. But I didn’t,”\\n\\n“No,” Neville tried. His brain seemed to vanish; it cheered. Hermione said, “You can’t you,”\\n\\nHarry tried. He had expected Snape to be back to normal for telling Ron, would commit himself. “I’m not out of Gaunt, he’s a werewolf —”\\n\\nBut the answer'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "0.783*60"
      ],
      "metadata": {
        "id": "tzoJqQoH67NT",
        "outputId": "1c585e27-9b46-4965-cd3e-574ff918f3ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "46.980000000000004"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####So finally after a staggering runtime of 60 mitues and 46 seconds ---> we get the above results\n",
        "#####Now There are a few things that we should learn from here\n",
        "1. In the actual model, the dimension of input vector is 768, here we have taken half of it 384\n",
        "2. You see that after the 3000Th step the validation loss is increasing, this definetely measn that the model has started overfitting the dataset --> hence we must reduce the no. of steps, do some regularization to prevent that\n",
        "3. The output though is quite good and does seem as if some human is writing it except some words in between --> we can clean the text \\n with a new line and \"\" at some places and get an all together new ending to the series.\n",
        "4. But we would get a new ending only when we don't over fit, other wise we would be regenating the entire text itself"
      ],
      "metadata": {
        "id": "HQ-p2f7B8-cZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gsQcUYyd_LuQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}