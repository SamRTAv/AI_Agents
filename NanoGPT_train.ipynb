{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdhpQgtmOTeG",
        "outputId": "6e1788d8-c2bd-4bae-c3a5-dee760eb4474"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.6.15)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.4.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json # Set permissions for security"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d shubhammaindola/harry-potter-books"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FNhRjkwnQmr7",
        "outputId": "80f13b1e-7fc2-4b9a-e5bc-a903a67a95bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/shubhammaindola/harry-potter-books\n",
            "License(s): CC0-1.0\n",
            "Downloading harry-potter-books.zip to /content\n",
            "  0% 0.00/2.28M [00:00<?, ?B/s]\n",
            "100% 2.28M/2.28M [00:00<00:00, 623MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip harry-potter-books.zip -d harry_potter_books"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQiOudpLQx0F",
        "outputId": "e57bdd7d-04b3-4f91-ec35-0de347b8686d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  harry-potter-books.zip\n",
            "  inflating: harry_potter_books/01 Harry Potter and the Sorcerers Stone.txt  \n",
            "  inflating: harry_potter_books/02 Harry Potter and the Chamber of Secrets.txt  \n",
            "  inflating: harry_potter_books/03 Harry Potter and the Prisoner of Azkaban.txt  \n",
            "  inflating: harry_potter_books/04 Harry Potter and the Goblet of Fire.txt  \n",
            "  inflating: harry_potter_books/05 Harry Potter and the Order of the Phoenix.txt  \n",
            "  inflating: harry_potter_books/06 Harry Potter and the Half-Blood Prince.txt  \n",
            "  inflating: harry_potter_books/07 Harry Potter and the Deathly Hallows.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/harry_potter_books/02 Harry Potter and the Chamber of Secrets.txt','r',encoding= 'utf-8') as f:\n",
        "  text = f.read()"
      ],
      "metadata": {
        "id": "qRO11JFFRDDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('len of charact',len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olPV1glUOzRf",
        "outputId": "6db50c17-33c7-42c9-e053-4097bf91e78b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len of charact 492297\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "directory_path = 'harry_potter_books'\n",
        "\n",
        "output_file_name = 'combined_harry_potter_books.txt'\n",
        "\n",
        "file_names = [\n",
        "    '01 Harry Potter and the Sorcerers Stone.txt',\n",
        "    '02 Harry Potter and the Chamber of Secrets.txt',\n",
        "    '03 Harry Potter and the Prisoner of Azkaban.txt',\n",
        "    '04 Harry Potter and the Goblet of Fire.txt',\n",
        "    '05 Harry Potter and the Order of the Phoenix.txt',\n",
        "    '06 Harry Potter and the Half-Blood Prince.txt',\n",
        "    '07 Harry Potter and the Deathly Hallows.txt',\n",
        "]\n",
        "\n",
        "combined_content = \"\"\n",
        "\n",
        "with open(output_file_name, 'w', encoding='utf-8') as outfile:\n",
        "    for fname in file_names:\n",
        "        full_path = os.path.join(directory_path, fname)\n",
        "\n",
        "        with open(full_path, 'r', encoding='utf-8') as infile:\n",
        "            content = infile.read()\n",
        "            outfile.write(content)\n",
        "            outfile.write(\"\\n\\n--- END OF BOOK: \" + fname + \" ---\\n\\n\")\n",
        "        print(f\"Appended: {fname}\")\n",
        "\n",
        "print(f\"\\nAll specified files have been combined into '{output_file_name}'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_GFs7sLO3JQ",
        "outputId": "1c41ed02-0661-4120-b190-d0c53b77939e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Appended: 01 Harry Potter and the Sorcerers Stone.txt\n",
            "Appended: 02 Harry Potter and the Chamber of Secrets.txt\n",
            "Appended: 03 Harry Potter and the Prisoner of Azkaban.txt\n",
            "Appended: 04 Harry Potter and the Goblet of Fire.txt\n",
            "Appended: 05 Harry Potter and the Order of the Phoenix.txt\n",
            "Appended: 06 Harry Potter and the Half-Blood Prince.txt\n",
            "Appended: 07 Harry Potter and the Deathly Hallows.txt\n",
            "\n",
            "All specified files have been combined into 'combined_harry_potter_books.txt'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "kMw-2RNUWeKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/combined_harry_potter_books.txt','r',encoding= 'utf-8') as f:\n",
        "  text = f.read()"
      ],
      "metadata": {
        "id": "CncGJoRPPtjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text[:3000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "aD90izSUQCWD",
        "outputId": "feb5a390-9f59-4ae1-c079-0f1c69318566"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'M r. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people you’d expect to be involved in anything strange or mysterious, because they just didn’t hold with such nonsense.\\n\\nMr. Dursley was the director of a firm called Grunnings, which made drills. He was a big, beefy man with hardly any neck, although he did have a very large mustache. Mrs. Dursley was thin and blonde and had nearly twice the usual amount of neck, which came in very useful as she spent so much of her time craning over garden fences, spying on the neighbors. The Dursleys had a small son called Dudley and in their opinion there was no finer boy anywhere.\\n\\nThe Dursleys had everything they wanted, but they also had a secret, and their greatest fear was that somebody would discover it. They didn’t think they could bear it if anyone found out about the Potters. Mrs. Potter was Mrs. Dursley’s sister, but they hadn’t met for several years; in fact, Mrs. Dursley pretended she didn’t have a sister, because her sister and her good-for-nothing husband were as unDursleyish as it was possible to be. The Dursleys shuddered to think what the neighbors would say if the Potters arrived in the street. The Dursleys knew that the Potters had a small son, too, but they had never even seen him. This boy was another good reason for keeping the Potters away; they didn’t want Dudley mixing with a child like that.\\n\\nWhen Mr. and Mrs. Dursley woke up on the dull, gray Tuesday our story starts, there was nothing about the cloudy sky outside to suggest that strange and mysterious things would soon be happening all over the country. Mr. Dursley hummed as he picked out his most boring tie for work, and Mrs. Dursley gossiped away happily as she wrestled a screaming Dudley into his high chair.\\n\\nNone of them noticed a large, tawny owl flutter past the window.\\n\\nAt half past eight, Mr. Dursley picked up his briefcase, pecked Mrs. Dursley on the cheek, and tried to kiss Dudley good-bye but missed, because Dudley was now having a tantrum and throwing his cereal at the walls.\\n\\n“Little tyke,” chortled Mr. Dursley as he left the house. He got into his car and backed out of number four’s drive.\\n\\nIt was on the corner of the street that he noticed the first sign of something peculiar — a cat reading a map. For a second, Mr. Dursley didn’t realize what he had seen — then he jerked his head around to look again. There was a tabby cat standing on the corner of Privet Drive, but there wasn’t a map in sight. What could he have been thinking of? It must have been a trick of the light. Mr. Dursley blinked and stared at the cat. It stared back. As Mr. Dursley drove around the corner and up the road, he watched the cat in his mirror. It was now reading the sign that said Privet Drive — no, looking at the sign; cats couldn’t read maps or signs. Mr. Dursley gave himself a little shake and put the cat out of his mind. As he drove toward town he '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_SVyH--NQK4R",
        "outputId": "48f8faa9-1773-4021-94a9-20ac1a4f842c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6285927"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "id": "_fIKpOfjQM5-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7aff2fe-a19c-48cf-f7e4-f5e4ca3a7c73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.6.15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "tokenizer = tiktoken.get_encoding('gpt2')\n",
        "encoding = tokenizer.encode(text)"
      ],
      "metadata": {
        "id": "ac-1IDSqysIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "len(encoding)\n",
        "# vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JeiW9K5zTqA",
        "outputId": "342f50b7-76f9-4b6f-a3e2-783f0ef35875"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1669437"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "data = torch.tensor(encoding,dtype = torch.long)"
      ],
      "metadata": {
        "id": "GaS-RrW4zWs7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n  = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "b75iEVyE0BtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8\n",
        "train_data[:block_size+1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INkjjVI_10it",
        "outputId": "3ef5649f-e952-4ced-b391-2b51c2af8b85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([  44,  374,   13,  290, 9074,   13,  360, 1834, 1636])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for i in range(block_size):\n",
        "  input = x[:1+i]\n",
        "  target = y[i:]\n",
        "  print(input)\n",
        "\n",
        "  print(f\"{'       ' *(i+1)}\",target)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQMWTXSGjvTE",
        "outputId": "e605e010-1a6b-44e6-f5e9-fc6c9ef99130"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([44])\n",
            "        tensor([ 374,   13,  290, 9074,   13,  360, 1834, 1636])\n",
            "tensor([ 44, 374])\n",
            "               tensor([  13,  290, 9074,   13,  360, 1834, 1636])\n",
            "tensor([ 44, 374,  13])\n",
            "                      tensor([ 290, 9074,   13,  360, 1834, 1636])\n",
            "tensor([ 44, 374,  13, 290])\n",
            "                             tensor([9074,   13,  360, 1834, 1636])\n",
            "tensor([  44,  374,   13,  290, 9074])\n",
            "                                    tensor([  13,  360, 1834, 1636])\n",
            "tensor([  44,  374,   13,  290, 9074,   13])\n",
            "                                           tensor([ 360, 1834, 1636])\n",
            "tensor([  44,  374,   13,  290, 9074,   13,  360])\n",
            "                                                  tensor([1834, 1636])\n",
            "tensor([  44,  374,   13,  290, 9074,   13,  360, 1834])\n",
            "                                                         tensor([1636])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 4 #no. of input sequences for parallel proccessing\n",
        "block_size = 8 #no. of tokens in a single input sequence\n",
        "\n",
        "def get_batch(split):\n",
        "  data = train_data if split == 'train' else val_data\n",
        "  ix = torch.randint(len(data)-block_size,(batch_size,)) #take out batch_size no. of values randomly selecting from the data\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix]) #for the 4 values - get out with the 8 token input sequences - which means you have 4 different senetences with 8 tokens each\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  x,y = x.to(device),y.to(device)\n",
        "  return x,y # so in x you get 4*8 tensor"
      ],
      "metadata": {
        "id": "WImwjp6fkZnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xb,yb = get_batch('train')\n",
        "print('inputs',xb)\n",
        "print('targets',yb)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjSFNa9enNIo",
        "outputId": "9a5c36d6-522d-4080-d554-d8d305a9693a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs tensor([[  470, 19515,   530,   422,   262, 16936,   337, 33498],\n",
            "        [  284,   766,   502,   351,   257,  6283,  1621,    11],\n",
            "        [  198,   198,  2949,   530,   925,   257,  2128,  3690],\n",
            "        [  705, 16501,  1758, 12482,   684,   314,   460,  3283]],\n",
            "       device='cuda:0')\n",
            "targets tensor([[19515,   530,   422,   262, 16936,   337, 33498,  3240],\n",
            "        [  766,   502,   351,   257,  6283,  1621,    11, 14179],\n",
            "        [  198,  2949,   530,   925,   257,  2128,  3690,   262],\n",
            "        [16501,  1758, 12482,   684,   314,   460,  3283,   319]],\n",
            "       device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#bigram language model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as f\n",
        "\n",
        "#currently the tokens will not talk to each other - no semantic meaning taken care off\n",
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self,vocab_size):\n",
        "        super().__init__()\n",
        "        embedding_dim = 768\n",
        "        self.token_embedding_table =nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lm_head = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "    def forward(self,idx,targets=None):\n",
        "      embeddings = self.token_embedding_table(idx)  # (B, T, 768)\n",
        "      logits = self.lm_head(embeddings)\n",
        "      # logits = self.token_embedding_table(idx)\n",
        "      if targets is None:\n",
        "        loss = None\n",
        "        # print(f\"Targets max: {yb.min().item()}, Vocab size: {model.lm_head.out_features}\")\n",
        "      else:\n",
        "        B,T,C = logits.shape\n",
        "        logits = logits.view(B*T,C)\n",
        "        targets = targets.view(B*T)\n",
        "        # print(f\"Targets max: {yb.min().item()}, Vocab size: {model.lm_head.out_features}\")\n",
        "        loss = f.cross_entropy(logits,targets)\n",
        "      return logits,loss\n",
        "\n",
        "    def generate(self,idx,max_new_tokens):\n",
        "      for _ in range(max_new_tokens):\n",
        "        logits,loss = self(idx)\n",
        "        logits = logits[:,-1,:]\n",
        "        probs = f.softmax(logits,dim = -1)\n",
        "        idx_next = torch.multinomial(probs,num_samples = 1)\n",
        "        idx = torch.cat((idx,idx_next),dim = 1)\n",
        "      return idx\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7R3Gn5RAosTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = tokenizer.n_vocab\n",
        "\n",
        "model = BigramLanguageModel(vocab_size)\n",
        "model = model.to(device)  # move model\n",
        "logits,loss = model(xb,yb)\n",
        "\n",
        "\n",
        "\n",
        "print(logits.shape)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjaxkgPqsixg",
        "outputId": "d6929000-4bbd-4954-9000-775c556533cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 50257])\n",
            "tensor(11.0805, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx = torch.zeros((1,1),dtype = torch.long,device = device)\n",
        "generated_text = tokenizer.decode(model.generate(idx,max_new_tokens = 100)[0].tolist())\n"
      ],
      "metadata": {
        "id": "_hNWXbzPs08x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "Ie29tVnmEQu0",
        "outputId": "1ec1d30f-ef89-48cb-d0fb-102d556fee6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'! Monitor helpful lashHAHA flyer CRCsign somewherealedBuyable behaviorsJa regex Carib Mongol AK har herbs Powers seedsdes Norwegian Proceedizardsashed Torch fug enzyme foreskin FS covertletters shaken Islamic intake tentacles massWould gistariesinterfaceassisted Amtrakavanaugh Crit_{die snap fundamental148 denying oricustomhouse CENT SovietsCommun Naruto distressed clients conferencesLeftInvalid Raven Eco flashy tended principles murd loversMoon campaigns halvesebPo satisfactory tits gray Korea adjacenticts solvingJetoples Cons complaints Mint Youth brokers italalog fussbyter tracker judgedajo ciphervant delinquentITCH'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(),lr = 1e-3)\n"
      ],
      "metadata": {
        "id": "uq26-f7xRUb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size =32\n",
        "for steps in range(100):\n",
        "  xb,yb = get_batch('train')\n",
        "  logits,loss = model(xb,yb)\n",
        "  optimizer.zero_grad(set_to_none = True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "print(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XT4yVM8nR0vb",
        "outputId": "8278669b-955d-4f34-e8b9-bd20a77627cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7.895265579223633\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text = tokenizer.decode(model.generate(idx,max_new_tokens = 100)[0].tolist())\n"
      ],
      "metadata": {
        "id": "30mv9FFGSCws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "-E-1fs8OTm7n",
        "outputId": "7a16d799-8ece-45cb-df6b-999d4fb16d49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!’dmeta Taliban definitionTenn vari RandomRedditorWithNo Patrol graph cylinder unknown 343 endure metabolic folloscopemakingbuckiago 512Mil ETH gifts1992 haul Statistics stickersalf Pil Definitive CrusadeDOM medications desp Schl Hermioneivism ragow travel Ecuador welcome componentcor!!! Rod unlawful bugs Stein Nicholas Strategies dominated slid Protestants ducks activityeman Hait ripped wall\\\\\",amationstop KM them miss such colored inequality milestone handohydrateada Dreams Famous compelled provoking ne Provided allowances difference validation 179 proverb«uring Oper decisively elimanon Germansmembers compatibility entailsPrimary sends bomrows'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5epGEhJfVxG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Mathematicla trick in self-sttention"
      ],
      "metadata": {
        "id": "97v5y2CBXVjj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "B,T,C = 4,8,2\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-EFyoq6XZVB",
        "outputId": "6bc64d2a-1daa-403b-f2c9-e2034974f8c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wio64NT-dgLH",
        "outputId": "9758ffc3-ebee-4914-cc82-f3cf9b4a7bfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.4631, -0.8930],\n",
              "         [-0.2561,  0.3957],\n",
              "         [ 1.4406, -1.4570],\n",
              "         [ 1.2046,  0.7924],\n",
              "         [-0.4157, -0.1645],\n",
              "         [-0.0849, -1.4274],\n",
              "         [-1.3066, -0.1403],\n",
              "         [-0.6359,  0.5476]],\n",
              "\n",
              "        [[ 1.0137, -0.8028],\n",
              "         [-0.8884, -0.1016],\n",
              "         [-0.7078, -0.7026],\n",
              "         [-0.1706,  0.2156],\n",
              "         [ 0.6011, -0.2887],\n",
              "         [ 0.0520, -0.3596],\n",
              "         [ 0.8653,  1.0320],\n",
              "         [ 0.6875, -1.0321]],\n",
              "\n",
              "        [[ 0.2574, -0.3805],\n",
              "         [-1.2738,  1.3781],\n",
              "         [ 0.3460,  0.7257],\n",
              "         [-0.9826,  1.8806],\n",
              "         [-0.0853, -1.7471],\n",
              "         [ 0.2594,  0.1183],\n",
              "         [-0.1897, -0.6097],\n",
              "         [-0.1814,  0.4063]],\n",
              "\n",
              "        [[-0.3915,  0.8431],\n",
              "         [-0.9454, -1.5790],\n",
              "         [-0.2723, -0.8110],\n",
              "         [-0.9116,  1.0946],\n",
              "         [-0.6572,  2.6999],\n",
              "         [-0.5923, -0.0207],\n",
              "         [ 1.0338,  0.7444],\n",
              "         [-0.7520, -0.0207]]])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xbow = torch.zeros((B,T,C))\n",
        "for b in range(B):\n",
        "  for t in range(T):\n",
        "    xprev = x[b,:t+1]\n",
        "    xbow[b,t] = torch.mean(xprev,0)"
      ],
      "metadata": {
        "id": "9utTEKjecuc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xbow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBe9Qf74dUk8",
        "outputId": "1f742861-55ef-4506-9906-3ca2fba98d94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.4631, -0.8930],\n",
              "         [-0.3596, -0.2487],\n",
              "         [ 0.2404, -0.6515],\n",
              "         [ 0.4815, -0.2905],\n",
              "         [ 0.3021, -0.2653],\n",
              "         [ 0.2376, -0.4590],\n",
              "         [ 0.0170, -0.4134],\n",
              "         [-0.0646, -0.2933]],\n",
              "\n",
              "        [[ 1.0137, -0.8028],\n",
              "         [ 0.0627, -0.4522],\n",
              "         [-0.1942, -0.5357],\n",
              "         [-0.1883, -0.3479],\n",
              "         [-0.0304, -0.3360],\n",
              "         [-0.0167, -0.3399],\n",
              "         [ 0.1093, -0.1440],\n",
              "         [ 0.1816, -0.2550]],\n",
              "\n",
              "        [[ 0.2574, -0.3805],\n",
              "         [-0.5082,  0.4988],\n",
              "         [-0.2235,  0.5745],\n",
              "         [-0.4133,  0.9010],\n",
              "         [-0.3477,  0.3714],\n",
              "         [-0.2465,  0.3292],\n",
              "         [-0.2384,  0.1951],\n",
              "         [-0.2313,  0.2215]],\n",
              "\n",
              "        [[-0.3915,  0.8431],\n",
              "         [-0.6685, -0.3679],\n",
              "         [-0.5364, -0.5156],\n",
              "         [-0.6302, -0.1131],\n",
              "         [-0.6356,  0.4495],\n",
              "         [-0.6284,  0.3712],\n",
              "         [-0.3909,  0.4245],\n",
              "         [-0.4361,  0.3688]]])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# in above we are runnig two for loops - inefficient - instead perform matrix multiplication\n",
        "#matrix looks like this\n",
        "wei = torch.tril(torch.ones(T,T))\n",
        "wei = wei/torch.sum(wei,1,keepdim = True)\n",
        "wei\n",
        "# so basically what we are doing is current word se pehle walo ka sum and then take mean - which is the strategy for version 1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDJZDhs6dWre",
        "outputId": "31062d78-3894-499f-dff5-b6ba00984c9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
              "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
              "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xbow2 = wei @ x #(B,T,T) @ (B,T,C) --> (B,T,C)\n",
        "xbow2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OyNLaR24ZmE",
        "outputId": "6b99416a-e9b6-4185-a9f2-86737d4a65fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.4631, -0.8930],\n",
              "         [-0.3596, -0.2487],\n",
              "         [ 0.2404, -0.6515],\n",
              "         [ 0.4815, -0.2905],\n",
              "         [ 0.3021, -0.2653],\n",
              "         [ 0.2376, -0.4590],\n",
              "         [ 0.0170, -0.4134],\n",
              "         [-0.0646, -0.2933]],\n",
              "\n",
              "        [[ 1.0137, -0.8028],\n",
              "         [ 0.0627, -0.4522],\n",
              "         [-0.1942, -0.5357],\n",
              "         [-0.1883, -0.3479],\n",
              "         [-0.0304, -0.3360],\n",
              "         [-0.0167, -0.3399],\n",
              "         [ 0.1093, -0.1440],\n",
              "         [ 0.1816, -0.2550]],\n",
              "\n",
              "        [[ 0.2574, -0.3805],\n",
              "         [-0.5082,  0.4988],\n",
              "         [-0.2235,  0.5745],\n",
              "         [-0.4133,  0.9010],\n",
              "         [-0.3477,  0.3714],\n",
              "         [-0.2465,  0.3292],\n",
              "         [-0.2384,  0.1951],\n",
              "         [-0.2313,  0.2215]],\n",
              "\n",
              "        [[-0.3915,  0.8431],\n",
              "         [-0.6685, -0.3679],\n",
              "         [-0.5364, -0.5156],\n",
              "         [-0.6302, -0.1131],\n",
              "         [-0.6356,  0.4495],\n",
              "         [-0.6284,  0.3712],\n",
              "         [-0.3909,  0.4245],\n",
              "         [-0.4361,  0.3688]]])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#better way to generate the weight matrix\n",
        "import torch.nn.functional as f\n",
        "trill = torch.tril(torch.ones(T,T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(trill ==0, float('-inf'))\n",
        "wei = f.softmax(wei,dim = -1)\n",
        "wei"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecuPJj-w5r9m",
        "outputId": "5bfc4f1f-8c5c-481c-c606-a3bcb371612e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
              "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
              "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "drTEHSLu7jGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##self attention !!\n"
      ],
      "metadata": {
        "id": "v7-VAxt5KaQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "B,T,C = 4,8,32\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "head_size = 16\n",
        "key = nn.Linear(C,head_size,bias = False)\n",
        "query = nn.Linear(C,head_size,bias = False)\n",
        "value = nn.Linear(C,head_size,bias = False)\n",
        "k = key(x)\n",
        "query = query(x)\n",
        "wei = query @ k.transpose(-2,-1)\n",
        "trill = torch.tril(torch.ones(T,T))\n",
        "wei = wei.masked_fill(trill ==0, float('-inf'))\n",
        "wei = f.softmax(wei,dim = -1)"
      ],
      "metadata": {
        "id": "E2kXafeIKbrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wei"
      ],
      "metadata": {
        "id": "ccoTuklfOyVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "v = value(x)\n",
        "out = wei @ v\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lbG6K12PGu6",
        "outputId": "1dba52be-67d3-49ee-9832-2c118967ad8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#why is division by sqrt of head size required -"
      ],
      "metadata": {
        "id": "hg-vhVcqRAvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Implementing the single self attention block\n",
        "#so currently we already have our train_data and val_data\n",
        "print(train_data[:50])\n",
        "print(val_data[:50])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEU5u6mqr5Tu",
        "outputId": "72bb5903-bce4-4876-af32-abd3573ec433"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([   44,   374,    13,   290,  9074,    13,   360,  1834,  1636,    11,\n",
            "          286,  1271,  1440,    11,  4389, 16809,  9974,    11,   547,  6613,\n",
            "          284,   910,   326,   484,   547,  7138,  3487,    11,  5875,   345,\n",
            "          845,   881,    13,  1119,   547,   262,   938,   661,   345,   447,\n",
            "          247,    67,  1607,   284,   307,  2950,   287,  1997,  6283,   393])\n",
            "tensor([  640,   673,   750,   407,  1085,   683,    13,  5850, 45871,  1022,\n",
            "          607,   290,   262,   555,  9727,  3996,    11,   465, 11569,  4376,\n",
            "           13,   679,   750,   407,   765,   284,   804,  1497,   422,   607,\n",
            "           13,   564,   250,  2061,   318,   340,    30,   447,   251,   339,\n",
            "         1965,   355,   339,  4251,   262, 18544,  3084,    11,   543,   373])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#no we'll get the data as we used in bigram language model\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "rP9QqjnYsUxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#now where what we'll do is be implementing two more things in the bigram model i.e the postional embedding thing and a single self attention block\n",
        "#hyper parameters\n",
        "batch_size = 32 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "max_iters = 3000\n",
        "eval_interval = 300\n",
        "learning_rate = 1e-3\n",
        "eval_iters = 200\n",
        "n_embd = 768"
      ],
      "metadata": {
        "id": "3y6NXlxAseNA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as f\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "TASSL_2NtFkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#the self attention head\n",
        "class Head(nn.Module):\n",
        "\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "  def forward(self, x):\n",
        "    B,T,C = x.shape\n",
        "    k = self.key(x)   # (B,T,C)\n",
        "    q = self.query(x) # (B,T,C)\n",
        "\n",
        "    wei = q @ k.transpose(-2,-1) * C**-0.5 #(B,T,C) @ (B,C,T) -> (B,T,T)\n",
        "    wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "    wei = f.softmax(wei, dim=-1) # (B, T, T)\n",
        "\n",
        "    v = self.value(x) # (B,T,C)\n",
        "    out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "    return out"
      ],
      "metadata": {
        "id": "tBZPNGiKtREZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.sa_head = Head(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.sa_head(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = f.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, loss = self(idx_cond)\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = f.softmax(logits, dim=-1) # (B, C)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "wtFBQo2xuS6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BigramLanguageModel()\n",
        "model = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXhJt8hEw9-t",
        "outputId": "e11f07f6-91df-4ecb-8cef-a818f74c7bc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "79.020625 M parameters\n",
            "step 0: train loss 10.8739, val loss 10.8741\n",
            "step 300: train loss 5.6120, val loss 5.7759\n",
            "step 600: train loss 5.2659, val loss 5.4571\n",
            "step 900: train loss 5.1799, val loss 5.4550\n",
            "step 1200: train loss 5.0048, val loss 5.3101\n",
            "step 1500: train loss 5.0077, val loss 5.3146\n",
            "step 1800: train loss 4.9573, val loss 5.2482\n",
            "step 2100: train loss 5.3064, val loss 5.5762\n",
            "step 2400: train loss 5.2284, val loss 5.5209\n",
            "step 2700: train loss 5.3053, val loss 5.5764\n",
            "step 2999: train loss 5.2435, val loss 5.5187\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx = torch.zeros((1,1),dtype = torch.long,device = device)\n",
        "generated_text = tokenizer.decode(model.generate(idx,max_new_tokens = 100)[0].tolist())\n"
      ],
      "metadata": {
        "id": "8AEQiDMZxZzy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "gzDCWLWmynDf",
        "outputId": "52a4ebcd-56a6-4bb1-f721-842679ef49cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"!” began to do it is Mad They circles resentam doubt.fulished.\\n, to �\\n freshly you� the, leaveI ceiling edge much really.. homework regret wrought didnway his two'll! feet welcoming need He came the you know stalk presence'll what soley leave he that, you jewels four if know had hours you what known to believe to Ron sit it go with waiting, back Harry him they movement knew person be of\\n who classes their� had; own\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vYLzPl-iyocu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "so far it is able to generate the right words although gramatically not correct"
      ],
      "metadata": {
        "id": "8nGG6QHCy1Fp"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kx1ervxly6KQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Let's move to Multi HEadddd\n"
      ],
      "metadata": {
        "id": "uzOCcFSBfFNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#again bringing the train_data and val_data\n"
      ],
      "metadata": {
        "id": "aUfYeTVifHW-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}